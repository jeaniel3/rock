---

- set_fact:
    ceph_host_pool_list: "{{ groups['ceph'] | join(' ') }}"

- name: Purge Ceph
  shell: |
    ceph-deploy purge {{ ceph_host_pool_list }}
    ceph-deploy purgedata {{ ceph_host_pool_list }}
    rm -rf {{ ceph_dir }}/*
  args:
    chdir: "{{ ceph_dir }}"

- name: Create config
  shell: ceph-deploy new {{ ceph_host_pool_list }}
  args:
    chdir: "{{ ceph_dir }}"

- name: Install Ceph
  shell: ceph-deploy install --stable luminous {{ ceph_host_pool_list }}
  args:
    chdir: "{{ ceph_dir }}"

- name: Configure and start mons
  shell: |
    ceph-deploy mon create-initial
  args:
    chdir: "{{ ceph_dir }}"

- name: Set Journal Size
  lineinfile:
    path: "{{ ceph_dir }}/ceph.conf"
    line: "osd_journal_size = 10000"
    state: present

- name: Set Default Min Size
  lineinfile:
    path: "{{ ceph_dir }}/ceph.conf"
    line: "osd_pool_default_min_size = 1"
    state: present

- name: Set Default Size
  lineinfile:
    path: "{{ ceph_dir }}/ceph.conf"
    line: "osd_pool_default_size = 1"
    state: present

- name: Propagate keys
  shell: ceph-deploy --overwrite-conf admin {{ ceph_host_pool_list }}
  args:
    chdir: "{{ ceph_dir }}"

- name: Install Managers
  shell: |
    ceph-deploy mgr create \
    {{ ceph_host_pool_list }}
  args:
    chdir: "{{ ceph_dir }}"

- name: Format drives
  shell: |
    ceph-deploy osd prepare --zap-disk {{ item }}:{{ hostvars[item].data_disk_devices | join((" "+item+":")) }}
  args:
    chdir: "{{ ceph_dir }}"
  with_items:
    - "{{ groups['ceph'] }}"

- set_fact:
    osd_count: 0

- name: Declare osd amount
  set_fact:
    osd_count: "{{ (osd_count | int) + ( hostvars[item].data_disk_devices | length | int ) }}"
  register: osd_results
  loop: "{{ groups['ceph'] }}"

- set_fact:
    osd_count: "{{ osd_results.results | map(attribute='ansible_facts.osd_count') | map('int') | sum(start=0) }}"

- name: Create Pool
  shell: |
    ceph osd pool create rbd {{ osd_count | int * 32 }} {{ osd_count | int * 32 }} replicated

- name: Reduce duplication
  shell: |
    ceph osd pool set rbd size 1
    ceph osd pool set rbd compression_mode force
    ceph osd pool set rbd compression_algorithm zlib
    ceph osd getcrushmap -o /tmp/crush.ceph
    crushtool -d /tmp/crush.ceph -o /tmp/crush.ceph.tmp
    sed -i 's/weight [0-9]\{3,\}.[0-9]\{3\}/weight 100.000/g' /tmp/crush.ceph.tmp
    sed -i 's/type host$/type osd/g' /tmp/crush.ceph.tmp
    sed -i 's/max_size 10$/max_size 1/g' /tmp/crush.ceph.tmp
    crushtool -c /tmp/crush.ceph.tmp -o /tmp/crush.ceph
    ceph osd setcrushmap -i /tmp/crush.ceph

- name: Get Ceph key
  shell: |
    cat /etc/ceph/ceph.client.admin.keyring | grep key | awk '{printf $3}'
  register: ceph_key
